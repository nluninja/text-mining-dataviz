{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d158749",
      "metadata": {
        "id": "6d158749"
      },
      "source": [
        "# GloVe\n",
        "In this notebook we are going to explain the concepts and use of word embeddings in NLP, using Glove as en example. Then we will try to apply the pre-trained Glove word embeddings to solve a text classification problem using this technique."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dee16a2",
      "metadata": {
        "id": "6dee16a2"
      },
      "source": [
        "## Large Movie Review Dataset\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.\n",
        "\n",
        "Link to dataset: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "Publications Using the Dataset:\n",
        "\n",
        "_Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd6d59a",
      "metadata": {
        "id": "6cd6d59a"
      },
      "source": [
        "## Loading the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8b26eac",
      "metadata": {
        "id": "a8b26eac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pickle\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#Import module to split the datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import modules to evaluate the metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a070c30",
      "metadata": {
        "id": "9a070c30"
      },
      "source": [
        "We set the variables for data location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5d624947",
      "metadata": {
        "id": "5d624947"
      },
      "outputs": [],
      "source": [
        "# Global parameters\n",
        "#root folder\n",
        "root_folder='.'\n",
        "data_folder_name='data'\n",
        "glove_filename='glove.6B.100d.txt'\n",
        "\n",
        "train_filename='train.csv'\n",
        "# Variable for data directory\n",
        "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
        "glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
        "\n",
        "# Both train and test set are in the root data directory\n",
        "train_path = DATA_PATH\n",
        "test_path = DATA_PATH\n",
        "\n",
        "#Relevant columns\n",
        "TEXT_COLUMN = 'text'\n",
        "TARGET_COLUMN = 'target'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e3fbbc",
      "metadata": {
        "id": "04e3fbbc"
      },
      "source": [
        "## Loading a pre-trained word embedding: GloVe\n",
        "Files with the pre-trained vectors Glove can be found in many sites like Kaggle or in the previous link of the Stanford University. We will use the glove.6B.100d.txt file containing the glove vectors trained on the Wikipedia and GigaWord dataset.\n",
        "* Glove Download:  https://nlp.stanford.edu/projects/glove/\n",
        "* Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download) https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "First we convert the GloVe file containing the word embeddings to the word2vec format for convenience of use. We can do it using the gensim library, a function called glove2word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f92143",
      "metadata": {
        "id": "24f92143",
        "outputId": "02648c8e-d450-4d94-dc02-08cb93da4556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-24 08:24:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-10-24 08:24:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-10-24 08:24:16--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/content/data/glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         93%[=================>  ] 766.40M  5.03MB/s    eta 12s    "
          ]
        }
      ],
      "source": [
        "# We just need to run this code once, the function glove2word2vec\n",
        "# saves the Glove embeddings in the word2vec format\n",
        "# that will be loaded in the next section\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "#glove_input_file = glove_filename\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "#glove2word2vec(glove_path, word2vec_output_file)\n",
        "\n",
        "# Download the GloVe embeddings if it doesn't exist\n",
        "if not os.path.exists(glove_path):\n",
        "  !wget http://nlp.stanford.edu/data/glove.6B.zip -P {DATA_PATH}\n",
        "  !unzip {DATA_PATH}/glove.6B.zip -d {DATA_PATH}\n",
        "\n",
        "# Convert GloVe to Word2Vec format\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "glove2word2vec(glove_path, word2vec_output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bdb6f9f",
      "metadata": {
        "id": "1bdb6f9f"
      },
      "source": [
        "So our vocabulary contains 400K words represented by a feature vector of shape 100. Now we can load the Glove embeddings in word2vec format and then analyze some analogies. In this way if we want to use a pre-trained word2vec embeddings we can simply change the filename and reuse all the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fec10ff",
      "metadata": {
        "id": "3fec10ff"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "\n",
        "#Show a word embedding\n",
        "print('King: ',model.get_vector('king'))\n",
        "\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "\n",
        "print('Most similar word to King + Woman - man: ', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3020cee",
      "metadata": {
        "id": "b3020cee"
      },
      "source": [
        "We would like extract some interesting features of our word embeddings,Now, our words are numerical vectors so we can measure and compare distances between words to show some of the properties that these embedding provide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c247c6ee",
      "metadata": {
        "id": "c247c6ee"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print('King - Man + Woman = ',result)\n",
        "result = model.most_similar(positive=['rome', 'france'], negative=['paris'], topn=1)\n",
        "print('France - Paris + Rome = ',result)\n",
        "result = model.most_similar(positive=['english', 'france'], negative=['french'], topn=1)\n",
        "print('France - french + english = ',result)\n",
        "result = model.most_similar(positive=['june', 'december'], negative=['november'], topn=1)\n",
        "print('December - November + June = ',result)\n",
        "result = model.most_similar(positive=['sister', 'man'], negative=['woman'], topn=1)\n",
        "print('Man - Woman + Sister = ',result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defffc24",
      "metadata": {
        "id": "defffc24"
      },
      "source": [
        "We can observe how the word vectors include information to relate countries with nationalities, months of the year, family relationships, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99873d86",
      "metadata": {
        "id": "99873d86"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['aunt', 'nephew'], negative=['niece'], topn=1)\n",
        "print('aunt - nephew + niece = ',result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230e67c1",
      "metadata": {
        "id": "230e67c1"
      },
      "source": [
        "We can extract wich words are more similar to another word, so they all are \"very close\" in the vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8d4010",
      "metadata": {
        "id": "0a8d4010"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['spain'], topn=10)\n",
        "print('10 most similar words to Spain: ',result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfee822",
      "metadata": {
        "id": "bbfee822"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['football'], topn=10)\n",
        "print('\\n10 most similar words to Football: ',result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599a3511",
      "metadata": {
        "id": "599a3511"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['doctor'], topn=10)\n",
        "print('\\n10 most similar words to Doctor: ',result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8374481",
      "metadata": {
        "id": "e8374481"
      },
      "outputs": [],
      "source": [
        "#Lets show some measure of similarities between words\n",
        "result = model.similar_by_word(\"cat\")\n",
        "print(\" Cat is similar to {}: {:.4f}\".format(*result[0]))\n",
        "result = model.similar_by_word(\"father\")\n",
        "print(\" Father is similar to {}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2e99ae",
      "metadata": {
        "id": "ec2e99ae"
      },
      "source": [
        "## Loading the dataset\n",
        "Out IMdb reviews can be loaded from keras.dataset. But in this dataset the tokens has already been \"numericalized\", but we want them as plain text, so we need to reverse them to the original tokens. Then, we load the dataset, load the vocabulary and we will use the vocab to get the original token for every single token in the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c523233",
      "metadata": {
        "id": "8c523233"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "!pip install keras==2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394067e1",
      "metadata": {
        "id": "394067e1"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=None)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "review = [reverse_word_index.get(i-3, \"\") for i in train_data[0]]\n",
        "review2 = [reverse_word_index.get(i-3, \"\") for i in train_data[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39f4af0",
      "metadata": {
        "id": "f39f4af0"
      },
      "outputs": [],
      "source": [
        "reverse_word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ef9563",
      "metadata": {
        "id": "c7ef9563"
      },
      "outputs": [],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743acf8c",
      "metadata": {
        "id": "743acf8c"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4edca820",
      "metadata": {
        "id": "4edca820"
      },
      "outputs": [],
      "source": [
        "print('Vocabulary lenght: ',len(word_index))\n",
        "print('Review example: ',review[:20])\n",
        "print('Label: ',train_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6985c9d",
      "metadata": {
        "id": "d6985c9d"
      },
      "outputs": [],
      "source": [
        "# Convert the train and test dataset to strings of words\n",
        "X_train=[]\n",
        "for doc in train_data:\n",
        "    X_train.append(' '.join([reverse_word_index.get(i - 3, \"\") for i in doc]))\n",
        "\n",
        "X_test=[]\n",
        "for doc in test_data:\n",
        "    X_test.append(' '.join([reverse_word_index.get(i - 3, \"\") for i in doc]))\n",
        "print(len(X_train),len(X_test))\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6a4169",
      "metadata": {
        "id": "ce6a4169"
      },
      "source": [
        "Applying the word embedding to a text classification task\n",
        "Now, we have our word representation, a vector for every word in our vocabulary. But we need to deal with full sentences so we need to create a **sentence embedding**, basically we need a vector that represent the whole sentence and every feature in the vector will be based on the word embeddings. There are many posibilities and we are notr going to cover this topic, so we apply a very simple method: the ith value in the sentence embedding will be the mean of the ith values in the word embedding of all the words in the sentence.\n",
        "\n",
        "We will create a class that will contain our vocabulary and glove vectors and then it will transform every review (a sentence in our example) to a vector representation as we describe previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eead1e6",
      "metadata": {
        "id": "2eead1e6"
      },
      "outputs": [],
      "source": [
        "class Word2VecVectorizer:\n",
        "  def __init__(self, model):\n",
        "    print(\"Loading in word vectors...\")\n",
        "    self.word_vectors = model\n",
        "    print(\"Finished loading in word vectors\")\n",
        "\n",
        "  def fit(self, data):\n",
        "    # build your word2vec here and then use it with your transform function\n",
        "    # In our case we keep the model that has been loaded with Glove\n",
        "    pass\n",
        "\n",
        "  def transform(self, data):\n",
        "    # determine the dimensionality of vectors\n",
        "    v = self.word_vectors.get_vector('king')\n",
        "    self.D = v.shape[0]\n",
        "    print (\"shape: \" + str(self.D))\n",
        "\n",
        "    X = np.zeros((len(data), self.D))\n",
        "    print (\"X shape:\" + str(np.shape(X)))\n",
        "    n = 0\n",
        "    emptycount = 0\n",
        "    for sentence in data:\n",
        "      tokens = sentence.split()\n",
        "      vecs = []\n",
        "      m = 0\n",
        "      for word in tokens:\n",
        "        try:\n",
        "          # throws KeyError if word not found\n",
        "          vec = self.word_vectors.get_vector(word)\n",
        "          vecs.append(vec)\n",
        "          m += 1\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if len(vecs) > 0:\n",
        "        vecs = np.array(vecs)\n",
        "        X[n] = vecs.mean(axis=0)\n",
        "        #print (\"X[\"+str(n)+\"]: shape: \" +str(np.shape(X[n])))\n",
        "        #print (X[n])\n",
        "      else:\n",
        "        emptycount += 1\n",
        "      n += 1\n",
        "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "    return X\n",
        "\n",
        "\n",
        "  def fit_transform(self, data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f2d32b",
      "metadata": {
        "id": "53f2d32b"
      },
      "source": [
        "Next, we create a Vectorizer object that will help us to transform our reviews to vectors, a numerical representation. Then we can use those vectors to feed our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e865ab",
      "metadata": {
        "id": "12e865ab"
      },
      "outputs": [],
      "source": [
        "# Set a word vectorizer\n",
        "vectorizer = Word2VecVectorizer(model)\n",
        "# Get the sentence embeddings for the train dataset\n",
        "Xtrain = vectorizer.fit_transform(X_train)\n",
        "Ytrain = train_labels\n",
        "# Get the sentence embeddings for the test dataset\n",
        "Xtest = vectorizer.transform(X_test)\n",
        "Ytest = test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4ba7e3",
      "metadata": {
        "id": "9b4ba7e3"
      },
      "outputs": [],
      "source": [
        "print(Xtrain.shape,Xtest.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e26cbfa",
      "metadata": {
        "id": "0e26cbfa"
      },
      "source": [
        "## Train a classifier on the sentence embeddings\n",
        "As text classification is just a type of classification problem we can apply some of the well-known classifiers to predict the label of a text. The next cells build different models to solve our classification task.\n",
        "\n",
        "But first we create some helper functions to plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae81ead6",
      "metadata": {
        "id": "ae81ead6"
      },
      "outputs": [],
      "source": [
        "# Create the confusion matrix\n",
        "def plot_confusion_matrix(y_test, y_pred):\n",
        "    ''' Plot the confussion matrix for the target labels and predictions '''\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Create a dataframe with the confussion matrix values\n",
        "    df_cm = pd.DataFrame(cm, range(cm.shape[0]),\n",
        "                  range(cm.shape[1]))\n",
        "    #plt.figure(figsize = (10,7))\n",
        "    # Plot the confussion matrix\n",
        "    sn.set(font_scale=1.4) #for label size\n",
        "    sn.heatmap(df_cm, annot=True,fmt='.0f',annot_kws={\"size\": 10})# font size\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cacd7f38",
      "metadata": {
        "id": "cacd7f38"
      },
      "source": [
        "## A random forest classifier\n",
        "First, we try with a simple model, a Random Forest. It can be considered as a baseline benchmark to any binary clasification problem.\n",
        "\n",
        "This notebook is about word embeddings, so we are not covering or interested in building the best classifier. We want to show us how to use the embeddings in a very simple way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e6787d",
      "metadata": {
        "id": "92e6787d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# create the model, train it, print scores\n",
        "clf = RandomForestClassifier(n_estimators=200)\n",
        "\n",
        "clf.fit(Xtrain, Ytrain)\n",
        "\n",
        "print(\"train score:\", clf.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", clf.score(Xtest, Ytest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9cdc11",
      "metadata": {
        "id": "1c9cdc11"
      },
      "outputs": [],
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = clf.predict(Xtest)\n",
        "\n",
        "print(metrics.classification_report(Ytest, y_pred,  digits=5))\n",
        "plot_confusion_matrix(Ytest, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f2d4f8",
      "metadata": {
        "id": "86f2d4f8"
      },
      "source": [
        "## ROC Curve\n",
        "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
        "\n",
        "* True Positive Rate\n",
        "* False Positive Rate\n",
        "\n",
        "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n",
        "\n",
        "![ROC](https://github.com/nluninja/text-mining-dataviz/blob/main/8.%20Embeddings/roc.png?raw=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272f23a3",
      "metadata": {
        "id": "272f23a3"
      },
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "# Calculate the points in the ROC curve\n",
        "def plot_roc_curve(y_test, y_pred):\n",
        "    ''' Plot the ROC curve for the target labels and predictions'''\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    roc_auc= auc(fpr,tpr)\n",
        "\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e7575d",
      "metadata": {
        "id": "b6e7575d"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve(Ytest, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2fd20c2",
      "metadata": {
        "id": "c2fd20c2"
      },
      "source": [
        "**AUC** stands for **Area under the ROC Curve.** That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03d4e0d",
      "metadata": {
        "id": "b03d4e0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}